{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fed719-1215-4547-a92a-abccf9e51d65",
   "metadata": {},
   "source": [
    "                                                WEB SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e4199-7965-4210-88c5-2c8020f82f88",
   "metadata": {},
   "source": [
    "1) Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fef8b4f8-7467-4c2b-83e7-1594c82026b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year of Release]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = requests.get(\"https://www.imdb.com/list/ls056092300/\")\n",
    "\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "yearsofrelease = []\n",
    "\n",
    "for movie in movies:\n",
    "    name = movie.h3.a.text.strip()\n",
    "    rating = movie.strong.text.strip()\n",
    "    year = movie.h3.find('span', class_='lister-item-year').text.strip()\n",
    "    names.append(title)\n",
    "    ratings.append(rating)\n",
    "    yearsofrelease.append(year)\n",
    "\n",
    "movies_df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Year of Release': yearsofrelease})\n",
    "\n",
    "print(movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552488e-cb5a-4797-9a89-1b38a33a20e2",
   "metadata": {},
   "source": [
    "2) Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efaa6c10-b27e-4794-b68b-1133909a31cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Heading, Date, Content, Likes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "contents = []\n",
    "likes = []\n",
    "\n",
    "posts = soup.find_all('article')\n",
    "\n",
    "for post in posts:\n",
    "    heading = post.find('h2').text.strip() if post.find('h2') else 'N/A'\n",
    "    headings.append(heading)\n",
    "    date = post.find('time')['datetime'] if post.find('time') else 'N/A'\n",
    "    dates.append(date)\n",
    "    content = post.find('div', class_='post-content').text.strip() if post.find('div', class_='post-content') else 'N/A'\n",
    "    contents.append(content)\n",
    "    like = post.find('span', class_='likes-count').text.strip() if post.find('span', class_='likes-count') else 'N/A'\n",
    "    likes.append(like)\n",
    "    \n",
    "posts_df = pd.DataFrame({\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'Content': contents,\n",
    "    'Likes': likes\n",
    "})\n",
    "\n",
    "print(posts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71392573-2d13-4d06-946b-32ca0b52f3fd",
   "metadata": {},
   "source": [
    "3) Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "986da908-c991-4e5a-8ee0-dc16555deda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for Indira Nagar\n",
      "Failed to retrieve data for Jayanagar\n",
      "Failed to retrieve data for Rajaji Nagar\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "base_url = 'https://www.nobroker.in/'\n",
    "\n",
    "results = []\n",
    "\n",
    "for locality in localities:\n",
    "    search_url = f\"{base_url}/property-for-rent-in-{locality.replace(' ', '-').lower()}\"\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        houses = soup.find_all('div', class_='property-card')\n",
    "        \n",
    "        for house in houses:\n",
    "            title = house.find('h2', class_='title').text.strip()\n",
    "            location = house.find('div', class_='location').text.strip()\n",
    "            area = house.find('div', class_='area').text.strip()\n",
    "            emi = house.find('div', class_='emi').text.strip()\n",
    "            price = house.find('div', class_='price').text.strip()\n",
    "\n",
    "            results.append({\n",
    "                'Title': title,\n",
    "                'Location': location,\n",
    "                'Area': area,\n",
    "                'EMI': emi,\n",
    "                'Price': price\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {locality}\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d35a1-e239-44a8-b6e9-8a3a0d5c36e0",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae1bb50-34c2-4d41-abea-d0df02a1014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', class_='productCard', limit=10) \n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for product in products:\n",
    "        name = product.find('h3', class_='productName').text.strip()\n",
    "        price = product.find('span', class_='discountedPrice').text.strip()\n",
    "        image_url = product.find('img')['src'] \n",
    "        results.append({\n",
    "            'Product Name': name,\n",
    "            'Price': price,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0d4bc-146b-4d96-91ed-447ba5ee1728",
   "metadata": {},
   "source": [
    "5) Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    "a) headings\n",
    "b) date\n",
    "c) News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb208747-0ad2-4b12-92ff-5c8d18091b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='Card', limit=10)\n",
    "    results = []\n",
    "    \n",
    "    for article in articles:\n",
    "        heading = article.find('a', class_='Card-title').text.strip()\n",
    "        date = article.find('time')['datetime'] if article.find('time') else 'No date available'\n",
    "        news_link = article.find('a', class_='Card-title')['href']\n",
    "        results.append({\n",
    "            'Heading': heading,\n",
    "            'Date': date,\n",
    "            'News Link': news_link\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1b3e4-1a7b-4b48-a618-82af534809e1",
   "metadata": {},
   "source": [
    "6) Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/ and scrapa) Paper title\n",
    "b) date\n",
    "c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bc87a0e-b76e-4f51-9dd7-e6015bdc5e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='article-item', limit=10)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.find('h3', class_='title').text.strip()\n",
    "        date = article.find('span', class_='date').text.strip()\n",
    "        authors = article.find('div', class_='authors').text.strip()\n",
    "        results.append({\n",
    "            'Paper Title': title,\n",
    "            'Date': date,\n",
    "            'Author(s)': authors\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
