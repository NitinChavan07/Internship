{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af51aea3-4900-4dbb-af0f-d11563a70281",
   "metadata": {},
   "source": [
    "WEB SCRAPING-ASSIGNMENT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057579d-d680-4aca-8df5-4405cbb8bf09",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030cd594-a695-40a8-8467-962becfec01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon:  guitar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 products for 'guitar':\n",
      "\n",
      "1. Naayaab Craft Guitar Xylophone with 5 Tones, Musical Toy for Kids with Child Safe Mallets, Best Educational Development Musical Kid Toy as Best Holiday/Birthday Gift for Your Mini Musicians, 5 Knocks\n",
      "2. East top Harmonica C, Diatonic Harmonica Key of C 10 Holes 20 Tones Mouth Organ Blues Harp Harmonica For Adults, Kids, Beginners, Professionals and Students\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def search_amazon(product):\n",
    "    # Go to Amazon.in\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "\n",
    "    # Find the search bar and enter the product name\n",
    "    search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.send_keys(Keys.RETURN)  # Hit Enter\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)  # Adjust time as needed for slower networks\n",
    "\n",
    "    # Fetch the product results\n",
    "    product_titles = driver.find_elements(By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "\n",
    "    print(f\"Found {len(product_titles)} products for '{product}':\\n\")\n",
    "    for index, title in enumerate(product_titles[:10]):  # Display the first 10 products\n",
    "        print(f\"{index + 1}. {title.text}\")\n",
    "\n",
    "# Take input from the user\n",
    "product_to_search = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "# Perform the search\n",
    "search_amazon(product_to_search)\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d384a2a-7362-4dd0-8a9d-2af064b74b98",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cea5d68-b352-4538-845d-ae696c2dc68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon:  guitar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to guitar_amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to extract product details from a single page\n",
    "def extract_product_details():\n",
    "    products = []\n",
    "\n",
    "    # Get all product containers\n",
    "    product_containers = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "    for container in product_containers:\n",
    "        try:\n",
    "            # Extract brand name\n",
    "            brand_name = container.find_element(By.XPATH, \".//span[@class='a-size-base-plus a-color-base']\").text\n",
    "        except NoSuchElementException:\n",
    "            brand_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract product name\n",
    "            product_name = container.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "        except NoSuchElementException:\n",
    "            product_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract price\n",
    "            price = container.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        except NoSuchElementException:\n",
    "            price = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract return/exchange information\n",
    "            return_exchange = container.find_element(By.XPATH, \".//span[contains(text(),'Free Return')]\").text\n",
    "        except NoSuchElementException:\n",
    "            return_exchange = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract expected delivery date\n",
    "            expected_delivery = container.find_element(By.XPATH, \".//span[contains(text(),'Get it by')]\").text\n",
    "        except NoSuchElementException:\n",
    "            expected_delivery = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract availability (availability tag not always present)\n",
    "            availability = container.find_element(By.XPATH, \".//span[contains(text(),'In stock')]\").text\n",
    "        except NoSuchElementException:\n",
    "            availability = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract product URL\n",
    "            product_url = container.find_element(By.XPATH, \".//a[@class='a-link-normal s-no-outline']\").get_attribute(\"href\")\n",
    "        except NoSuchElementException:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        # Append the details to the list\n",
    "        products.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Name of the Product\": product_name,\n",
    "            \"Price\": price,\n",
    "            \"Return/Exchange\": return_exchange,\n",
    "            \"Expected Delivery\": expected_delivery,\n",
    "            \"Availability\": availability,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "# Function to scrape multiple pages of products\n",
    "def scrape_amazon(product, num_pages=3):\n",
    "    all_products = []\n",
    "    \n",
    "    # Open Amazon and search for the product\n",
    "    driver.get(\"https://www.amazon.in\")\n",
    "    search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "    search_box.send_keys(product)\n",
    "    search_box.submit()\n",
    "\n",
    "    # Iterate through the pages and scrape the data\n",
    "    for page in range(1, num_pages + 1):\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        # Extract details from the current page\n",
    "        all_products.extend(extract_product_details())\n",
    "\n",
    "        try:\n",
    "            # Go to the next page\n",
    "            next_page = driver.find_element(By.XPATH, \"//a[contains(@class, 's-pagination-next')]\")\n",
    "            next_page.click()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Less than {num_pages} pages found. Scraping completed.\")\n",
    "            break\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# Main function to start scraping and save data to a CSV\n",
    "def main():\n",
    "    # Take input from the user\n",
    "    product_to_search = input(\"Enter the product to search on Amazon: \")\n",
    "\n",
    "    # Scrape product details for the first 3 pages (or less if fewer pages exist)\n",
    "    scraped_data = scrape_amazon(product_to_search, num_pages=3)\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = f\"{product_to_search}_amazon_products.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de745cf3-373b-407a-b5aa-20f1e349838e",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed155c1-1962-4f24-baa7-6701520f89e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images for: fruits\n",
      "Scraping images for: cars\n",
      "Scraping images for: Machine Learning\n",
      "Scraping images for: Guitar\n",
      "Scraping images for: Cakes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to download and save images\n",
    "def download_images(image_urls, folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for i, url in enumerate(image_urls):\n",
    "        try:\n",
    "            image_content = requests.get(url).content\n",
    "            image_file = BytesIO(image_content)\n",
    "            image = Image.open(image_file)\n",
    "            image_filename = os.path.join(folder_name, f\"{folder_name}_{i + 1}.jpg\")\n",
    "            image.save(image_filename, \"JPEG\")\n",
    "            print(f\"Downloaded {image_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download image {i + 1}: {e}\")\n",
    "\n",
    "# Function to scrape images from Google Images for a given search term\n",
    "def scrape_google_images(search_term, num_images=10):\n",
    "    search_url = f\"https://images.google.com/?q={search_term}\"\n",
    "    driver.get(search_url)\n",
    "\n",
    "    # Accept Google consent pop-up if it appears\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        accept_button = driver.find_element(By.XPATH, \"//button[contains(text(),'I agree')]\")\n",
    "        accept_button.click()\n",
    "    except:\n",
    "        pass  # If no pop-up appears, continue scraping\n",
    "\n",
    "    # Find the search bar and input the search term\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(search_term)\n",
    "    search_box.submit()\n",
    "\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Scroll down to load more images (optional)\n",
    "    for _ in range(2):  # Scroll multiple times to load more images\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Collect image URLs\n",
    "    image_elements = driver.find_elements(By.CSS_SELECTOR, \"img.rg_i\")\n",
    "    image_urls = []\n",
    "    for image_element in image_elements[:num_images]:  # Only take the first `num_images` images\n",
    "        try:\n",
    "            image_element.click()\n",
    "            time.sleep(1)  # Allow time for the larger image to load\n",
    "            larger_image_element = driver.find_element(By.CSS_SELECTOR, \"img.n3VNCb\")\n",
    "            image_url = larger_image_element.get_attribute(\"src\")\n",
    "            if image_url and \"http\" in image_url:\n",
    "                image_urls.append(image_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while fetching image: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Download and save the images\n",
    "    download_images(image_urls, search_term)\n",
    "\n",
    "# Main function to scrape images for different search terms\n",
    "def main():\n",
    "    search_terms = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "\n",
    "    for term in search_terms:\n",
    "        print(f\"Scraping images for: {term}\")\n",
    "        scrape_google_images(term, num_images=10)\n",
    "\n",
    "    # Close the browser after scraping\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bd736-dc6f-4911-9b4c-364b186e8dfe",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3734a352-c87f-4b60-a50b-96a7ba1e8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the smartphone to search on Flipkart:  samsung mobile \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to samsung mobile _flipkart_smartphones.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to extract smartphone details\n",
    "def extract_smartphone_details():\n",
    "    smartphones = []\n",
    "    \n",
    "    # Get all smartphone containers\n",
    "    product_containers = driver.find_elements(By.XPATH, \"//div[@class='_1AtVbE']\")\n",
    "    \n",
    "    for container in product_containers:\n",
    "        try:\n",
    "            # Extract the smartphone name\n",
    "            smartphone_name = container.find_element(By.XPATH, \".//a[@class='IRpwTa']\").text\n",
    "        except NoSuchElementException:\n",
    "            smartphone_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract brand name\n",
    "            brand_name = smartphone_name.split()[0]  # First word is often the brand\n",
    "        except:\n",
    "            brand_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract color and RAM/Storage information\n",
    "            desc = container.find_element(By.XPATH, \".//ul[@class='_1xgFaf']\").text\n",
    "            details = desc.split(\"\\n\")\n",
    "            color = details[0].split(\", \")[-1]\n",
    "            ram = details[1].split(\"|\")[0].strip()\n",
    "            storage = details[1].split(\"|\")[1].strip()\n",
    "        except NoSuchElementException:\n",
    "            color, ram, storage = \"-\", \"-\", \"-\"\n",
    "\n",
    "        try:\n",
    "            # Extract primary and secondary camera\n",
    "            cameras = details[2].split(\"|\")\n",
    "            primary_camera = cameras[0].strip()\n",
    "            secondary_camera = cameras[1].strip() if len(cameras) > 1 else \"-\"\n",
    "        except:\n",
    "            primary_camera, secondary_camera = \"-\", \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract display size\n",
    "            display_size = details[3].strip()\n",
    "        except:\n",
    "            display_size = \"-\"\n",
    "\n",
    "        try:\n",
    "            # Extract battery capacity\n",
    "            battery = details[4].strip() if len(details) > 4 else \"-\"\n",
    "        except:\n",
    "            battery = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract price\n",
    "            price = container.find_element(By.XPATH, \".//div[@class='_30jeq3']\").text.replace(\"₹\", \"\")\n",
    "        except NoSuchElementException:\n",
    "            price = \"-\"\n",
    "\n",
    "        try:\n",
    "            # Extract product URL\n",
    "            product_url = container.find_element(By.XPATH, \".//a[@class='_1fQZEK']\").get_attribute(\"href\")\n",
    "        except NoSuchElementException:\n",
    "            product_url = \"-\"\n",
    "\n",
    "        # Append the scraped details to the list\n",
    "        smartphones.append({\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Smartphone Name\": smartphone_name,\n",
    "            \"Colour\": color,\n",
    "            \"RAM\": ram,\n",
    "            \"Storage(ROM)\": storage,\n",
    "            \"Primary Camera\": primary_camera,\n",
    "            \"Secondary Camera\": secondary_camera,\n",
    "            \"Display Size\": display_size,\n",
    "            \"Battery Capacity\": battery,\n",
    "            \"Price\": price,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "\n",
    "    return smartphones\n",
    "\n",
    "# Function to scrape smartphone details from Flipkart\n",
    "def scrape_flipkart(smartphone, num_pages=1):\n",
    "    # Open Flipkart and search for the smartphone\n",
    "    driver.get(\"https://www.flipkart.com\")\n",
    "    time.sleep(2)  # Allow page to load\n",
    "    \n",
    "    # Close login pop-up if it appears\n",
    "    try:\n",
    "        close_login = driver.find_element(By.XPATH, \"//button[contains(text(), '✕')]\")\n",
    "        close_login.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    # Enter the smartphone into the search bar\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(smartphone)\n",
    "    search_box.submit()\n",
    "    time.sleep(2)  # Allow search results to load\n",
    "    \n",
    "    # Scrape the details from the first page\n",
    "    smartphones = extract_smartphone_details()\n",
    "\n",
    "    return smartphones\n",
    "\n",
    "# Main function to scrape and save data into a CSV\n",
    "def main():\n",
    "    # Take input from the user\n",
    "    smartphone_to_search = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "\n",
    "    # Scrape smartphone details from the first page\n",
    "    scraped_data = scrape_flipkart(smartphone_to_search)\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = f\"{smartphone_to_search}_flipkart_smartphones.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55a540-0a20-49cd-9538-6ddaa9d3da24",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5e35125-f803-4b4c-b5ec-00349d81dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the city name:  chh sambhaji nagar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates of chh sambhaji nagar:\n",
      "Latitude: 19.8836224\n",
      "Longitude: 75.2975872\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to get latitude and longitude from Google Maps\n",
    "def get_geospatial_coordinates(city_name):\n",
    "    # Open Google Maps\n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "    time.sleep(2)  # Allow page to load\n",
    "    \n",
    "    # Search for the city\n",
    "    search_box = driver.find_element(By.ID, \"searchboxinput\")\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.submit()\n",
    "    time.sleep(3)  # Wait for the search results to load\n",
    "    \n",
    "    # Get the URL with the coordinates in it\n",
    "    current_url = driver.current_url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extract the latitude and longitude from the URL\n",
    "    try:\n",
    "        # The URL contains a string like \"@12.9715987,77.5945627\" for the coordinates\n",
    "        coords_section = current_url.split('@')[1].split(',')[:2]\n",
    "        latitude, longitude = coords_section[0], coords_section[1]\n",
    "        print(f\"Coordinates of {city_name}:\")\n",
    "        print(f\"Latitude: {latitude}\")\n",
    "        print(f\"Longitude: {longitude}\")\n",
    "    except IndexError:\n",
    "        print(f\"Could not extract coordinates for {city_name}.\")\n",
    "    \n",
    "# Main function to run the script\n",
    "def main():\n",
    "    city = input(\"Enter the city name: \")\n",
    "    get_geospatial_coordinates(city)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdd0c9-188b-4762-96d6-3e16e7fb8513",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "938eba12-8371-4c09-af91-7d2d04cfb126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to best_gaming_laptops_digit.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to extract gaming laptop details\n",
    "def extract_laptop_details():\n",
    "    laptops = []\n",
    "    \n",
    "    # Load the Digit.in page for the best gaming laptops\n",
    "    driver.get(\"https://www.digit.in/top-products/best-gaming-laptops-40.html\")\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    # Use BeautifulSoup to parse the page source\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all laptop containers\n",
    "    laptop_containers = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "    \n",
    "    for container in laptop_containers:\n",
    "        try:\n",
    "            # Extract laptop name\n",
    "            laptop_name = container.find('h3').text.strip()\n",
    "        except:\n",
    "            laptop_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract laptop specifications\n",
    "            laptop_specs = container.find('div', class_='Specs').text.strip()\n",
    "        except:\n",
    "            laptop_specs = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract laptop price\n",
    "            laptop_price = container.find('div', class_='Price').text.strip().replace('₹', '')\n",
    "        except:\n",
    "            laptop_price = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract more details about the laptop\n",
    "            details_link = container.find('a', href=True)['href']\n",
    "            product_url = f\"https://www.digit.in{details_link}\"\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "        \n",
    "        # Append the scraped details to the list\n",
    "        laptops.append({\n",
    "            \"Laptop Name\": laptop_name,\n",
    "            \"Specifications\": laptop_specs,\n",
    "            \"Price\": laptop_price,\n",
    "            \"Product URL\": product_url\n",
    "        })\n",
    "\n",
    "    return laptops\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    # Scrape laptop details\n",
    "    laptop_data = extract_laptop_details()\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(laptop_data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = \"best_gaming_laptops_digit.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47131c93-c53a-4812-9c7e-5631a7fb08bb",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1ff981b-7e18-4ea9-8cbe-371a3c2e39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to forbes_billionaires.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "# Function to scrape billionaire details\n",
    "def extract_billionaire_details():\n",
    "    billionaires = []\n",
    "    \n",
    "    # Load the Forbes Billionaires page\n",
    "    driver.get(\"https://www.forbes.com/billionaires/\")\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "    \n",
    "    # Scroll the page to load all billionaires\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new content to load\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    # Use BeautifulSoup to parse the loaded page\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all billionaire containers\n",
    "    billionaire_containers = soup.find_all('div', class_='personName')\n",
    "    \n",
    "    for container in billionaire_containers:\n",
    "        try:\n",
    "            # Extract the rank\n",
    "            rank = container.find('div', class_='rank').text.strip()\n",
    "        except:\n",
    "            rank = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract the name\n",
    "            name = container.find('div', class_='personName').text.strip()\n",
    "        except:\n",
    "            name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract the net worth\n",
    "            net_worth = container.find('div', class_='netWorth').text.strip()\n",
    "        except:\n",
    "            net_worth = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract the age\n",
    "            age = container.find('div', class_='age').text.strip()\n",
    "        except:\n",
    "            age = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract citizenship\n",
    "            citizenship = container.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        except:\n",
    "            citizenship = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract source of wealth\n",
    "            source = container.find('div', class_='source').text.strip()\n",
    "        except:\n",
    "            source = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract the industry\n",
    "            industry = container.find('div', class_='category').text.strip()\n",
    "        except:\n",
    "            industry = \"-\"\n",
    "        \n",
    "        # Append the scraped details to the list\n",
    "        billionaires.append({\n",
    "            \"Rank\": rank,\n",
    "            \"Name\": name,\n",
    "            \"Net Worth\": net_worth,\n",
    "            \"Age\": age,\n",
    "            \"Citizenship\": citizenship,\n",
    "            \"Source\": source,\n",
    "            \"Industry\": industry\n",
    "        })\n",
    "\n",
    "    return billionaires\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    # Scrape billionaire details\n",
    "    billionaire_data = extract_billionaire_details()\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(billionaire_data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = \"forbes_billionaires.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850b731-1a30-4173-8b27-76282de39f63",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b323b870-1eb2-4e4b-8915-99aa4ae29576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the YouTube video URL:  https://www.youtube.com/watch?v=gPpQNzQP6gE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to scroll and load comments\n",
    "def scroll_and_load_comments():\n",
    "    # Scroll to load comments\n",
    "    driver.execute_script(\"window.scrollTo(0, 600);\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Keep scrolling until at least 500 comments are loaded\n",
    "    comment_count = 0\n",
    "    while comment_count < 500:\n",
    "        # Scroll down by a certain amount\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        comment_count = len(driver.find_elements(By.XPATH, '//*[@id=\"content-text\"]'))\n",
    "\n",
    "# Function to extract comments and details\n",
    "def extract_comments():\n",
    "    comments = []\n",
    "    \n",
    "    # Get all comment elements\n",
    "    comment_elements = driver.find_elements(By.XPATH, '//*[@id=\"content-text\"]')\n",
    "    upvote_elements = driver.find_elements(By.XPATH, '//*[@id=\"vote-count-middle\"]')\n",
    "    time_elements = driver.find_elements(By.XPATH, '//*[@id=\"header-author\"]/yt-formatted-string/a')\n",
    "    \n",
    "    # Iterate over the comments and extract details\n",
    "    for i in range(min(500, len(comment_elements))):\n",
    "        try:\n",
    "            comment_text = comment_elements[i].text\n",
    "        except:\n",
    "            comment_text = \"-\"\n",
    "        \n",
    "        try:\n",
    "            upvotes = upvote_elements[i].text if upvote_elements[i].text else \"0\"\n",
    "        except:\n",
    "            upvotes = \"-\"\n",
    "        \n",
    "        try:\n",
    "            time_posted = time_elements[i].text\n",
    "        except:\n",
    "            time_posted = \"-\"\n",
    "        \n",
    "        # Append to the list\n",
    "        comments.append({\n",
    "            \"Comment\": comment_text,\n",
    "            \"Upvotes\": upvotes,\n",
    "            \"Time Posted\": time_posted\n",
    "        })\n",
    "    \n",
    "    return comments\n",
    "\n",
    "# Main function to scrape YouTube comments\n",
    "def scrape_youtube_comments(video_url):\n",
    "    # Load the YouTube video page\n",
    "    driver.get(video_url)\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "    \n",
    "    # Scroll and load more comments\n",
    "    scroll_and_load_comments()\n",
    "    \n",
    "    # Extract comments and details\n",
    "    comments_data = extract_comments()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(comments_data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = \"youtube_comments.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    scrape_youtube_comments(video_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d1c1b-09f9-4742-8b8e-0dfa9fa1cc62",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8365ca73-5612-41ee-9fd4-0052fc628f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to hostels_in_london.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Function to extract hostel details\n",
    "def extract_hostel_details():\n",
    "    hostels = []\n",
    "    \n",
    "    # Load the Hostelworld page for London\n",
    "    driver.get(\"https://www.hostelworld.com/hostels/london\")\n",
    "    time.sleep(5)  # Allow the page to load\n",
    "    \n",
    "    # Scroll to load more hostels\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    # Find all hostel containers\n",
    "    hostel_containers = driver.find_elements(By.CLASS_NAME, 'property-card')\n",
    "    \n",
    "    for container in hostel_containers:\n",
    "        try:\n",
    "            # Extract hostel name\n",
    "            name = container.find_element(By.CLASS_NAME, 'property-card-title').text.strip()\n",
    "        except:\n",
    "            name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract distance from city center\n",
    "            distance = container.find_element(By.CLASS_NAME, 'distance').text.strip()\n",
    "        except:\n",
    "            distance = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract ratings\n",
    "            rating = container.find_element(By.CLASS_NAME, 'score').text.strip()\n",
    "        except:\n",
    "            rating = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract total reviews\n",
    "            total_reviews = container.find_element(By.CLASS_NAME, 'reviews').text.strip()\n",
    "        except:\n",
    "            total_reviews = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract overall reviews\n",
    "            overall_reviews = container.find_element(By.CLASS_NAME, 'overall-reviews').text.strip()\n",
    "        except:\n",
    "            overall_reviews = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract privates from price\n",
    "            private_price = container.find_element(By.XPATH, './/span[contains(@class, \"private-price\")]').text.strip()\n",
    "        except:\n",
    "            private_price = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract dorms from price\n",
    "            dorm_price = container.find_element(By.XPATH, './/span[contains(@class, \"dorm-price\")]').text.strip()\n",
    "        except:\n",
    "            dorm_price = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract facilities\n",
    "            facilities = container.find_element(By.CLASS_NAME, 'facilities').text.strip()\n",
    "        except:\n",
    "            facilities = \"-\"\n",
    "        \n",
    "        try:\n",
    "            # Extract property description\n",
    "            description = container.find_element(By.CLASS_NAME, 'property-card-description').text.strip()\n",
    "        except:\n",
    "            description = \"-\"\n",
    "        \n",
    "        # Append the scraped details to the list\n",
    "        hostels.append({\n",
    "            \"Hostel Name\": name,\n",
    "            \"Distance from City Centre\": distance,\n",
    "            \"Ratings\": rating,\n",
    "            \"Total Reviews\": total_reviews,\n",
    "            \"Overall Reviews\": overall_reviews,\n",
    "            \"Privates from Price\": private_price,\n",
    "            \"Dorms from Price\": dorm_price,\n",
    "            \"Facilities\": facilities,\n",
    "            \"Property Description\": description\n",
    "        })\n",
    "\n",
    "    return hostels\n",
    "\n",
    "# Main function to scrape and save data\n",
    "def main():\n",
    "    # Scrape hostel details\n",
    "    hostel_data = extract_hostel_details()\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(hostel_data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = \"hostels_in_london.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Scraped data saved to {csv_filename}\")\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
